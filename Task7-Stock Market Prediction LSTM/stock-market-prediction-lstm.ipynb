{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\ndf = []\nstock_name = []\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        #print(os.path.join(dirname, filename))\n        df.append(pd.read_csv(os.path.join(dirname, filename),index_col = 0) )\n        stock_name.append(filename)\n        #print(dirname)\n        #print(filename)\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n# index_col = 0 makes the Date column as the index else it was 0,1,2...","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-04-02T13:16:35.030223Z","iopub.execute_input":"2023-04-02T13:16:35.031519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[10].head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stock_name[100]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[100].shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[100].describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[100].info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[100][\"Adj Close\"].plot()\nplt.ylabel(\"Adj Close\")\nplt.xlabel(None)\nplt.title(f\"Closing Price of {stock_name[100]}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ma_day = [10, 50, 100, 365]\n\nfor ma in ma_day:\n    for company in df:\n        column_name = f\"MA for {ma} days\"\n        #Rolling calculates mean over ma days in time series\n        company[column_name] = company[\"Adj Close\"].rolling(ma).mean()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axes = plt.subplots(nrows=2, ncols=5)\nfig.set_figheight(8)\nfig.set_figwidth(30)\nk = 0; j = 0;\nfor i in range(100,110):\n    dataframes[i][['Adj Close', 'MA for 10 days', 'MA for 50 days', 'MA for 100 days','MA for 365 days']].plot(ax=axes[k,j]).set_title(f\"{stock_name[i]}\");\n    j = j+1;\n    if(j==5):\n        k=1;\n        j=0;\nfig.tight_layout()\n# linestyle = '--', marker = 'o'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for company in df:\n    company[\"Daily Return\"] = company[\"Adj Close\"].pct_change()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axes = plt.subplots(nrows = 2, ncols = 5)\nfig.set_figheight(8)\nfig.set_figwidth(30)\n\nk = 0\nj = 0\nfor i in range(100, 110):\n    df[i][[\"Daily Return\"]].plot(ax = axes[k, j], linestyle = \"--\", marker = \"o\").set_title(f\"{stock_name[i]}\")\n    j += 1\n    if (j == 5):\n        k = 1\n        j = 0\nfig.tight_layout()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense, LSTM\nimport tensorflow as tf","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_training_dataset(input_ds):\n    # Create a new dataframe with only the 'Close column \n    input_ds.reset_index()\n    data = input_ds.filter(items=['Close'])\n    # Convert the dataframe to a numpy array\n    dataset = data.values\n    # Get the number of rows to train the model on\n    training_data_len = int(np.ceil( len(dataset) * .95 ))\n    return data, dataset, training_data_len\n\n#Test the function\ntraining_data_df, training_dataset_np, training_data_len = build_training_dataset(dataframes[100])\ndataset=training_dataset_np\ndata=training_data_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\ndef scale_the_data(dataset):\n    scaler = MinMaxScaler(feature_range=(0,1))\n    scaled_data = scaler.fit_transform(dataset)\n    return scaler, scaled_data\n\n#Test the function\nscaler, scaled_data = scale_the_data(training_dataset_np)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create the training data set \n# Create the scaled training data set\ndef split_train_dataset(training_data_len):\n    train_data = scaled_data[0:int(training_data_len), :]\n    # Split the data into x_train and y_train data sets\n    x_train = []\n    y_train = []\n    for i in range(60, len(train_data)):\n        x_train.append(train_data[i-60:i, 0])\n        y_train.append(train_data[i, 0])\n        if i<= 61:\n            #print(x_train)\n            #print(y_train)\n            print('.')\n            \n    # Convert to numpy arrays \n    x_train, y_train = np.array(x_train), np.array(y_train)\n\n    # Reshape the data\n    x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1))\n    # x_train.shape\n    return x_train, y_train\n\n#Test the function\nx_train,y_train = split_train_dataset(training_data_len)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_lstm_model(x_train,y_train):\n    # Build the LSTM model\n    model = Sequential()\n    model.add(LSTM(128, return_sequences=True, input_shape= (x_train.shape[1], 1)))\n    model.add(LSTM(64, return_sequences=False))\n    model.add(Dense(25))\n    model.add(Dense(1))\n\n    # Compile the model\n    # adam ~ Stochastic Gradient descent method.\n    model.compile(optimizer='adam', loss='mean_squared_error')\n    # Train the model\n    model.fit(x_train, y_train, batch_size=1, epochs=1)\n    return model \n\n#Test the function\nlstm_model = build_lstm_model(x_train,y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##import pickle\n##pickle.dump(lstm_model, open(stock_name[100], 'wb'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_testing_data_set(model, scaler, training_data_len,test_data_len):\n    # Create the testing data set\n    # Create a new array containing scaled values from index 1543 to 2002 \n    test_data = scaled_data[training_data_len - test_data_len: , :]\n    # Create the data sets x_test and y_test\n    x_test = []\n    y_test = dataset[training_data_len:, :]\n    for i in range(test_data_len, len(test_data)):\n        x_test.append(test_data[i-test_data_len:i, 0])\n    \n    # Convert the data to a numpy array\n    x_test = np.array(x_test)\n\n    # Reshape the data\n    x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], 1 ))\n\n    # Get the models predicted price values \n    predictions = model.predict(x_test)\n    predictions = scaler.inverse_transform(predictions)\n\n    # Get the root mean squared error (RMSE)\n    rmse = np.sqrt(np.mean(((predictions - y_test) ** 2)))\n    rmse\n    return (x_test, y_test, predictions, rmse)\n\n#Test the function\nTEST_DATA_LENGTH = 100\nx_test,y_test, predictions, rmse = create_testing_data_set(lstm_model,scaler,training_data_len, TEST_DATA_LENGTH)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_predictions(stock, data,training_data_len):\n    #Plot the data\n    train = data[:training_data_len]\n    valid = data[training_data_len:]\n    valid['Predictions'] = predictions\n    # Visualize the data\n    plt.figure(figsize=(16,6))\n    title = stock + ' Model Forecast'\n    ylabel = stock + ' Close Price'\n    plt.title(title)\n    plt.xlabel('Date', fontsize=18)\n    plt.ylabel(ylabel, fontsize=18)\n    plt.plot(train['Close'])\n    plt.plot(valid[['Close', 'Predictions']])\n    plt.legend(['Training Data', 'Validated Data', 'Predicted Data'], loc='lower right')\n    plt.show()\n    return valid\n    \n#Test the function\nvalid = plot_predictions(stock_name[100],data,training_data_len)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from math import sqrt\nfrom sklearn.metrics import mean_squared_error\ni = 0\nTEST_DATA_LENGTH = 111\nerror_scores = {}\ntrained_model = {}\nfor stock in stock_name:\n    df= dataframes[i]\n    dataframes[i].dropna(inplace=True)\n    i= i+1\n    training_data_df, training_dataset_np, training_data_len = build_training_dataset(df) #Build the Training Dataset\n    dataset=training_dataset_np\n    data=training_data_df\n    scaler, scaled_data = scale_the_data(training_dataset_np) #Scale the data\n    x_train,y_train = split_train_dataset(training_data_len) #split the data\n    lstm_model = build_lstm_model(x_train,y_train) #build the LSTM model\n    trained_model[stock] = lstm_model\n    x_test,y_test, predictions, rmse = create_testing_data_set(lstm_model,scaler,training_data_len, TEST_DATA_LENGTH ) #create testing dataset and predictions\n    valid = plot_predictions(stock,data,training_data_len) #plot predictions\n    valid   # Show the valid and predicted prices\n    rmse = sqrt(mean_squared_error(valid['Close'], valid['Predictions']))\n    print('Test RMSE: %.3f' % (rmse))\n    #error_scores.append(rmse)\n    error_scores[stock] = rmse\n\nprint(error_scores)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}